{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3182840-d33e-41b3-8b2f-ac5b70661365",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fdf7d",
   "metadata": {},
   "source": [
    "# This notebook is modified for GEC task.\n",
    "\n",
    "Anocha S.\n",
    "6/4/2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1bb7b2-058b-4880-865f-3beec9ad2ccd",
   "metadata": {
    "id": "4b1bb7b2-058b-4880-865f-3beec9ad2ccd"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70a360-76ab-43e5-89bc-113db0a1b55c",
   "metadata": {},
   "source": [
    "# Data Preparation: Machine Translation\n",
    "\n",
    "Machine translation in NLP refers to the use of computer algorithms and models to automatically translate text or speech from one language to another. It is a subfield of NLP that focuses on developing systems that can bridge the language barrier and facilitate communication between people who speak different languages. Machine translation systems aim to replicate the process of human translation by analyzing the structure, grammar, and meaning of the source language text and generating an equivalent translation in the target language. These systems employ various techniques, including statistical methods, rule-based approaches, and more recently, neural machine translation (NMT) models.\n",
    "\n",
    "* **Statistical machine translation (SMT)** was the dominant approach before the rise of neural machine translation. SMT involves training models on large bilingual corpora, extracting patterns, and using statistical algorithms to generate translations. However, SMT often requires extensive manual feature engineering and may struggle with translating rare or unseen phrases.\n",
    "\n",
    "* **Neural machine translation (NMT)** has emerged as a powerful approach, leveraging deep learning techniques to improve translation quality. NMT models, particularly sequence-to-sequence models with recurrent or transformer architectures, learn to directly map source language sentences to target language sentences. They do not rely on explicit alignment models or handcrafted linguistic features, enabling end-to-end learning.\n",
    "\n",
    "NMT models are trained on parallel corpora, which consist of pairs of sentences in the source and target languages. During training, the models learn to encode the source sentence into a continuous representation, often using an encoder network, and then decode this representation into the target language sentence using a decoder network. The models are optimized to minimize the difference between the generated translation and the reference translation in the training data. There have been significant advancements in recent years, but challenges still remain, particularly in handling ambiguous or context-dependent language, translating idioms, and capturing nuances of meaning. Ongoing research and advancements in NLP continue to push the boundaries of machine translation capabilities.\n",
    "\n",
    "In this notebook, we will create a dataset to train machine learning models using publicly available corpora. The scale of this dataset will be rather limited and certainly insufficient to train a state-of-the-art machine translation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97defac-a2f9-4783-99c2-c297fd6649cd",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c5fdd5-c7ab-4a8a-9b4e-136a05c96f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1ff6d-a14c-4c4a-ab80-51c569dc464a",
   "metadata": {},
   "source": [
    "We utilize some utility methods from PyTorch as well as Torchtext, so we need to import the `torch` and `torchtext` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a52d60d-b8e9-4e6d-aea6-3cd5a8afaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3407d10-1c11-4622-9c99-d9b2c677379f",
   "metadata": {},
   "source": [
    "As usual, we rely on spacy to perform basic text preprocessing and cleaning steps. Note that we have to load 2 language models, one for the source and one for the target language, which will be German and English in this notebook, at least by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2e2711-3259-4cc5-87d1-0a8a4db5cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Tell spaCy to use the GPU (if available)\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp_eng = spacy.load(\"en_core_web_trf\")\n",
    "# nlp_deu = spacy.load(\"de_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c3bf9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c8af158-37ac-45c7-9c65-c10b497a8ade",
   "metadata": {},
   "source": [
    "Lastly, `src/utils.py` provides some utility methods to download and decompress files. Since the datasets used in some of the notebooks are of considerable size -- although far from huge -- they are not part of the repository and need to be downloaded (and optionally decompressed) separately. The 2 methods `download_file` and `decompress_file` accomplish this for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12c1dfa-8b33-4aa2-ae16-b9b719e514a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import download_file, decompress_file, get_line_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe1cd997",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dup2pairTxt' from 'convert2pair' (/mnt/c/Users/nomar/OneDrive - National University of Singapore/CS4248 NLP/Project/ChrisRNN/convert2pair.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_630/2870750895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconvert2pair\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mc2pairTxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdup2pairTxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'dup2pairTxt' from 'convert2pair' (/mnt/c/Users/nomar/OneDrive - National University of Singapore/CS4248 NLP/Project/ChrisRNN/convert2pair.py)"
     ]
    }
   ],
   "source": [
    "from convert2pair import c2pairTxt, dup2pairTxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f80e9a-631f-40e9-ad3e-9ff84ab5e285",
   "metadata": {},
   "source": [
    "Below we set the target path where to stored and find all datafiles. Since the files are quite large, they are nor part of the repository by need to be downloaded first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67d1e387-a513-48ec-bc04-9839e31f8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to GEC path\n",
    "# target_path = 'data/corpora/tatoeba/'\n",
    "# target_path = 'data/corpora/sentence_pairV2/'\n",
    "# target_path = 'data/corpora/Awe.datasets_binary/'\n",
    "target_path = 'data/corpora/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bbf2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# convert json sentence pair to csv txt file\n",
    "c2pairTxt(target_path,'ABC.train.gold.bea19') #jsonPath, jsonName\n",
    "c2pairTxt(target_path,'ABCN.dev.gold.bea19') #jsonPath, jsonName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup2pairTxt(testPath, testName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ca1a9-e094-4733-82cf-e5d3416d5f6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c632ed7-68a8-404b-a46f-92787ff466e0",
   "metadata": {},
   "source": [
    "## Download & Generate Dataset\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Training a machine learning model typically requires a large dataset of text documents and their corresponding translation in the one or more target languages. Collecting corpora for training machine translation models involves several steps. Here's an overview of the process:\n",
    "\n",
    "* **Determine Source and Target Languages:** First, identify the specific languages you want to train your machine translation model on. The choice of languages depends on your target audience and the availability of resources in those languages.\n",
    "\n",
    "* **Obtain Parallel Corpora:** Parallel corpora are collections of sentences or texts that have translations available in both the source and target languages. These corpora are essential for training machine translation models. There are several sources to consider:\n",
    "\n",
    "\t* Publicly Available Corpora: Explore publicly available parallel corpora, such as those provided by research institutions, organizations, or language-related projects. Some examples include the Europarl corpus, United Nations documents, or the Tatoeba project.\n",
    "\n",
    "\t* Government and Legal Translations: Government websites, legislative documents, legal agreements, and court proceedings often have translated versions available. These can be a valuable resource for specific domains or language pairs.\n",
    "\n",
    "\t* News Articles and Publications: News organizations and publishers may have translated articles or publications that can be used as parallel corpora. This can provide a diverse range of topics and sentence structures.\n",
    "\n",
    "\t* Crowdsourcing: Consider utilizing crowdsourcing platforms to collect translations. Platforms like Amazon Mechanical Turk or specialized translation communities can help in gathering sentence pairs for your target language pair.\n",
    "\n",
    "* **Ensure Data Quality and Preprocessing:** After obtaining parallel corpora, it is important to ensure data quality and perform preprocessing steps. This includes:\n",
    "\n",
    "\t* Removing noisy or irrelevant data: Review the data and remove any sentences or segments that are low quality, incorrect, or contain undesirable characteristics.\n",
    "\n",
    "\t* Tokenization: Tokenize sentences into individual words or subword units. This step is crucial for building vocabulary and preparing data for input to the machine translation model.\n",
    "\n",
    "\t* Cleaning and Normalization: Normalize the data by removing unnecessary punctuation, correcting spelling mistakes, or handling special characters specific to the languages.\n",
    "\n",
    "* **Align Sentences:** For training machine translation models, it is crucial to align the sentences in the parallel corpora, i.e., to establish which sentence in the source language corresponds to which sentence in the target language. Alignment can be done manually or with the help of alignment tools such as FastAlign or GIZA++.\n",
    "\n",
    "* **Corpus Size and Balance:** Consider the size of your corpus. Larger corpora can provide better coverage and generalization. Additionally, ensure the balance between the source and target languages, so that both languages have roughly equal representation to avoid bias in translation quality.\n",
    "\n",
    "* **Pretraining and Fine-tuning:** Machine translation models, particularly neural network-based models, often benefit from pretraining on a large dataset and then fine-tuning on a domain-specific or smaller dataset. This allows the model to learn general language patterns before focusing on the specific translation task.\n",
    "\n",
    "It is important to note that collecting corpora for machine translation can be a complex and time-consuming process, especially for low-resource languages or specialized domains. The availability of quality parallel corpora directly affects the translation quality of the trained models. Therefore, it is essential to invest effort into obtaining high-quality and diverse corpora for effective machine translation training.\n",
    "\n",
    "### Data Source: Tatoeba\n",
    "\n",
    "In this notebook we rely on [Tatoeba](https://tatoeba.org/en/) to collect our text corpus for generating our dataset(s). The Tatoeba website is a collaborative online platform that aims to collect and provide example sentences and translations in multiple languages. The word \"tatoeba\" means \"for example\" in Japanese, reflecting the purpose of the platform—to provide examples for various languages and contexts. The website's main goal is to create a large and diverse sentence database that can be used for language learning, translation, and linguistic research.\n",
    "\n",
    "Users of Tatoeba can contribute by submitting new sentences in any language, along with their translations into other languages. The sentences can cover a wide range of topics, allowing learners and researchers to explore different domains and language usage. The website follows a community-driven approach, where registered users can suggest corrections, discuss translations, and engage in collaborative efforts to improve the quality and accuracy of the sentence database.\n",
    "\n",
    "Tatoeba provides various features and tools to facilitate language exploration and learning. Users can search for sentences, filter by language, browse through curated lists, and save their favorite sentences. The translations provided on Tatoeba are typically contributed by volunteers, so the quality may vary, but the community actively works on improving and reviewing the translations over time.\n",
    "\n",
    "The Tatoeba project promotes open data and open-source principles. The sentence database and its source code are freely available, allowing others to reuse and build upon them. This openness enables researchers, developers, and language enthusiasts to create applications, tools, and resources that leverage the sentence data for diverse language-related tasks.\n",
    "\n",
    "### Auxiliary Method for Data Collection\n",
    "\n",
    "Tatoeba makes all sentences of a language available as a single compressed file. However, downloading these 2 files for the source and target language is not sufficient as they lack the connection which sentence in the source file matches which sentences in the target file. To make this connection requires an additional file containing the information about the links between sentences of different languages. The method `generate_sentence_pairs()` in the code cell below automates this process. It takes the identifiers of the source and target language as input and creates a new text file containing all matching sentence pairs between the two languages. While the first half of the method code handles downloading and decompressing the required files, the second half performs the linking between the 2 language files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64faffde-2af7-4b85-ba50-4ecf6db85cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # No need to call this since we have all sentence_pairs form Laing\n",
    "\n",
    "# def generate_sentence_pairs(src_lang, tgt_lang, target_path, overwrite=False):\n",
    "#     output_file_name = target_path+'ABC.train.gold.bea19-{}-{}.txt'.format(src_lang, tgt_lang)\n",
    "    \n",
    "#     # Check if file exists; onoverwriterwrite if specified\n",
    "#     if os.path.isfile(output_file_name) == True and overwrite is not True:\n",
    "#         print('Output file \"{}\" already exists.'.format(output_file_name))\n",
    "#         return output_file_name\n",
    "    \n",
    "#     print('Download files...')\n",
    "#     raw_src = download_file('https://downloads.tatoeba.org/exports/per_language/{}/{}_sentences.tsv.bz2'.format(src_lang, src_lang), target_path, overwrite=overwrite)\n",
    "#     raw_tgt = download_file('https://downloads.tatoeba.org/exports/per_language/{}/{}_sentences.tsv.bz2'.format(tgt_lang, tgt_lang), target_path, overwrite=overwrite)    \n",
    "#     raw_lnk = download_file('https://downloads.tatoeba.org/exports/links.tar.bz2', target_path, overwrite=overwrite)\n",
    "    \n",
    "#     print('Decompress files...')\n",
    "#     src = decompress_file(raw_src, target_path)\n",
    "#     tgt = decompress_file(raw_tgt, target_path)\n",
    "#     lnk = decompress_file('data/corpora/tatoeba/links.tar.bz2', target_path)\n",
    "#     lnk= decompress_file('data/corpora/tatoeba/links.tar', target_path)\n",
    "    \n",
    "#     print('Link language files...')\n",
    "#     df_src = pd.read_csv(src, sep='\\t', header=None)\n",
    "#     df_tgt = pd.read_csv(tgt, sep='\\t', header=None)\n",
    "#     df_links = pd.read_csv(target_path+'links.csv', sep='\\t', header=None)\n",
    "    \n",
    "#     src_ids = set(df_src[0])\n",
    "#     tgt_ids = set(df_tgt[0])\n",
    "    \n",
    "#     df_links = df_links[df_links[0].isin(src_ids) & df_links[1].isin(tgt_ids)]\n",
    "    \n",
    "#     num_pairs = len(df_links)\n",
    "    \n",
    "#     print('Generate output file...')\n",
    "#     output_file = open(output_file_name, 'w')\n",
    "#     with tqdm(total=num_pairs) as progress_bar:\n",
    "#         for index, row in df_links.iterrows():\n",
    "#             try:\n",
    "#                 src_row = df_src[df_src[0] == row[0]].to_records(index=False)[0]\n",
    "#                 tgt_row = df_tgt[df_tgt[0] == row[1]].to_records(index=False)[0]\n",
    "#                 output_file.write('{}\\t{}\\n'.format(src_row[2], tgt_row[2]))\n",
    "#             except Exception as e:\n",
    "#                 pass\n",
    "#             finally:\n",
    "#                 progress_bar.update(1)\n",
    "    \n",
    "#     print('DONE')\n",
    "#     return output_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ac1a1-e725-4b95-9b93-6a415fdc9f5c",
   "metadata": {},
   "source": [
    "In this and subsequent notebooks, by default, our goal is to build and train a machine translation model for translating German into English sentences. This means that our source language is German (Tatoeba identifier: `deu`) and our target language is English (Tatoeba identifier: `eng`). Let's call `generate_sentence_pairs()` to download all required files and prepare our dataset file containing match sentence pairs.\n",
    "\n",
    "**Important:** If you look at the code of `generate_sentence_pairs()` it assumes that the language files are accessible via certain URLs. In principle, these URLs might change overtime. So if the code cell below throws an error indicating that the URLs are invalid, we recommend to go to the Tatoeba website to check for the new URLs and update the method above accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9b54769-0ac2-4b98-9a88-552e92a1cfa1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "0.00iB [00:00, ?iB/s]"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/corpora/sentence_pairV2/src_sentences.tsv.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434/3053976282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# No need to call this since we have all sentence_pairs form Laing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sentence_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tgt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The methods returns the file name of our dataset, as we need that later to read the file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_434/1368498896.py\u001b[0m in \u001b[0;36mgenerate_sentence_pairs\u001b[0;34m(src_lang, tgt_lang, target_path, overwrite)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Download files...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mraw_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://downloads.tatoeba.org/exports/per_language/{}/{}_sentences.tsv.bz2'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mraw_tgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://downloads.tatoeba.org/exports/per_language/{}/{}_sentences.tsv.bz2'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mraw_lnk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://downloads.tatoeba.org/exports/links.tar.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/nomar/OneDrive - National University of Singapore/CS4248 NLP/Project/ChrisRNN/src/utils.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(url, target_path, overwrite)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_size_in_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Fetch file and write to path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/corpora/sentence_pairV2/src_sentences.tsv.bz2'"
     ]
    }
   ],
   "source": [
    "# # No need to call this since we have all sentence_pairs form Laing\n",
    "# dataset_file_name = generate_sentence_pairs('src', 'tgt', target_path, overwrite=False)\n",
    "\n",
    "# # The methods returns the file name of our dataset, as we need that later to read the file.\n",
    "# print(dataset_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ba88d-e622-4375-a505-f3986346bbc2",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "### Auxiliary Methods\n",
    "\n",
    "The code cell below defines 2 auxiliary methods to \"preprocess\" the sentences, respective to their language. Since the task is machine translation, the preprocessing is rather minimal, and here limited to lowercasing the tokenization. Other steps such as stopword removal or lemmatization are of course not appropriate here.\n",
    "\n",
    "Keep in mind that in practice, there are many additional steps conceivable. For example, one can replace number with some placeholder token and replace this token with the number in the translation. This is also often done with named entities such as the names of people or locations as they are commonly not translated, and this is an easy way to limited to size of the vocabularies. But as usual, the goals is not to build a state-of-the-art translation model, so we ignore such more sophisticated considerations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90611890-b5a0-4365-8df8-e2748530392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_eng(text):\n",
    "    return [token.text.lower() for token in nlp_eng.tokenizer(text)]\n",
    "\n",
    "# def tokenize_deu(text):\n",
    "#     return [token.text.lower() for token in nlp_deu.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a260a98e-573f-4334-97eb-820a5334c15d",
   "metadata": {},
   "source": [
    "### Create Vocabularies\n",
    "\n",
    "In previous Data Preparation notebooks, we already went multiple times to the basic steps of creating a vocabulary and vectorizing a corpus of text documents. We therefore keep it short in this notebook and put all the required code into a single code cell. But again, all this code should look very familiar if you went through earlier notebooks where we prepared a dataset for tasks such as sentiment analysis of language models.\n",
    "\n",
    "The main difference here is that we need to create 2 vocabularies, one for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "952cd3c7-2d62-47b8-b2cb-6c8781e09946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4477/4477 [00:00<00:00, 8689.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Src vocabulary: 7675\n",
      "Size of Tgt vocabulary: 7675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## preparing .vocab from sentence_pair.txt\n",
    "\n",
    "\n",
    "## output_file_name = target_path+'tatoeba-{}-{}.txt'.format(src_lang, tgt_lang)\n",
    "# input sentence_pair path\n",
    "dataset_file_name = target_path + 'ABCN.test.bea19.pairtxt'#'valid.pairstxt' #'ABC.train.gold.bea19.txt'#'ABCN.dev.gold.bea19.txt' \n",
    "## Create Counter to get word frequencies for Source and Target\n",
    "token_counter_src = Counter()\n",
    "token_counter_tgt = Counter()\n",
    "\n",
    "num_samples = get_line_count(dataset_file_name)\n",
    "\n",
    "## Read file line by line\n",
    "with open(dataset_file_name) as file:\n",
    "    with tqdm(total=num_samples) as t:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                # The German sentence comes first, then the English sentence\n",
    "                src, tgt = line.split(\"\\t\")\n",
    "                # Update German token counts\n",
    "                for token in tokenize_eng(src):\n",
    "                    token_counter_src[token] += 1\n",
    "                # Update English token counts\n",
    "                for token in tokenize_eng(tgt):\n",
    "                    token_counter_tgt[token] += 1\n",
    "            except:\n",
    "                pass\n",
    "            finally:\n",
    "                # Update progress bar\n",
    "                t.update(1)\n",
    "\n",
    "## Sort word frequencies and conver to an OrderedDict\n",
    "token_counter_src_sorted = sorted(token_counter_src.items(), key=lambda x: x[1], reverse=True)\n",
    "token_counter_tgt_sorted = sorted(token_counter_tgt.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Limited the maximum size of the vocabulary (note that 20k is kind of arbitrary,\n",
    "# and it's also not obvious which we should use the same value for both languages)\n",
    "max_words = 20000\n",
    "token_ordered_src_dict = OrderedDict(token_counter_src_sorted[:max_words])\n",
    "token_ordered_tgt_dict = OrderedDict(token_counter_tgt_sorted[:max_words])\n",
    "\n",
    "# Create vocabularies for EN and DE (note that we add a couple of special tokens you might have not seen yet,\n",
    "# but you can ignore them here as they don't harm training our RNN-based encoder-decoder model)\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "CLS_TOKEN = \"<CLS>\"\n",
    "SEP_TOKEN = \"<SEP>\"\n",
    "\n",
    "SPECIALS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN, CLS_TOKEN, SEP_TOKEN]\n",
    "\n",
    "# Create vocab objects\n",
    "vocab_src = vocab(token_ordered_src_dict, specials=SPECIALS)\n",
    "vocab_tgt = vocab(token_ordered_tgt_dict, specials=SPECIALS)\n",
    "\n",
    "# Set index of default token (i.e., the index that gets returned in case of unknown words)\n",
    "vocab_src.set_default_index(vocab_src[UNK_TOKEN])\n",
    "vocab_tgt.set_default_index(vocab_tgt[UNK_TOKEN])\n",
    "\n",
    "print(\"Size of Src vocabulary: {}\".format(len(vocab_src)))\n",
    "print(\"Size of Tgt vocabulary: {}\".format(len(vocab_tgt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f185d0-e2fe-4dbe-a02d-da76ec8ddc14",
   "metadata": {},
   "source": [
    "As usual, we need to save both vocabularies for later use (i.e., for when we want to train our model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5774f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8cc0fe7-c2af-4abb-ad7e-a0eda65c71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_src_file_name = target_path+ 'test-src.vocab' #'ABC.train-src-{}.vocab'.format(max_words)\n",
    "vocab_tgt_file_name = target_path+ 'test-tgt.vocab' #'ABC.train-tgt-{}.vocab'.format(max_words)\n",
    "\n",
    "torch.save(vocab_tgt, vocab_tgt_file_name)\n",
    "torch.save(vocab_src, vocab_src_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e332c3-b3e4-416c-8288-e44c01e4d1d0",
   "metadata": {},
   "source": [
    "### Vectorize Sentences\n",
    "\n",
    "With both vocabularies, we can now vectorize all sentences in the source and target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99bc0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In case of unseen sentences, keep using train vocab index\n",
    "src_path = 'data/corpora/Awe.datasets_binary/'\n",
    "vocab_src = torch.load( src_path+ 'train-src.vocab')# \"ABC.train-src-20000.vocab\")\n",
    "vocab_tgt = torch.load( src_path+ 'train-tgt.vocab')#\"ABC.train-tgt-20000.vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5130cfd-34b9-413f-ba20-aac88ce5a5e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/corpora/test/ABCN.test.bea19'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_630/1007462586.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_file_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/corpora/test/ABCN.test.bea19'"
     ]
    }
   ],
   "source": [
    "dataset_file_name = target_path + 'ABCN.test.bea19.pairtxt'#\"ABC.train.gold.bea19.txt\"\n",
    "output_file = open(target_path + 'test-vectorized.txt', 'w')#'ABC.train-src-tgt-vectorized.txt', \"w\")\n",
    "num_samples = get_line_count(dataset_file_name)\n",
    "\n",
    "\n",
    "with open(dataset_file_name) as file:\n",
    "    with tqdm(total=num_samples) as t:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            try:\n",
    "                src, tgt = line.split(\"\\t\")\n",
    "                # Vectorize both texts \n",
    "                src_vec = vocab_src.lookup_indices(tokenize_eng(src))\n",
    "                tgt_vec = vocab_tgt.lookup_indices(tokenize_eng(tgt))  \n",
    "#                 print(src,tgt)\n",
    "                # Write both texts to the output file (use tab as separator)\n",
    "                output_file.write(\"{}\\t{}\\n\".format(\" \".join([str(idx) for idx in src_vec]), \" \".join([str(idx) for idx in tgt_vec])))        \n",
    "            except:\n",
    "                # Detect error in sentence_pair file\n",
    "                # for removing them from m2 coz we need to assert size of m2 and our hypothesis(output)\n",
    "                print(\"====\\nRejecting ...\\n\",line,\"====\")\n",
    "                \n",
    "                pass\n",
    "            finally:\n",
    "                # Update progress bar\n",
    "                t.update(1)\n",
    "            \n",
    "output_file.flush()\n",
    "output_file.close()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723f011-a934-49e7-8601-11b69911f03f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Collecting and preparing datasets for training machine translation models is a crucial step in building high-quality translation systems. The process involves sourcing parallel corpora, which consist of sentences or texts in the source language along with their translations in the target language. These corpora can be obtained from various sources such as public repositories, government documents, news articles, or through crowdsourcing.\n",
    "\n",
    "Once the parallel corpora are acquired, preprocessing steps are applied to clean and normalize the data. This includes tokenization to split sentences into words or subword units, lowercasing, handling special characters, and cleaning the text by removing noise, punctuation, and irrelevant information. Sentence alignment ensures that each sentence in the source language corresponds correctly to its translation in the target language.\n",
    "\n",
    "Furthermore, data augmentation techniques can be employed to increase the dataset size and improve model performance. Techniques like back-translation and sentence length filtering can be applied to augment and filter the dataset, respectively. These preprocessing steps ensure that the data is in a suitable format for training the machine translation models, enabling effective learning of language patterns and translation mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cdece-1b78-4f7f-9355-547cdf988b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
