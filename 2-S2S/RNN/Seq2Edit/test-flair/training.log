2024-04-03 19:46:41,189 ----------------------------------------------------------------------------------------------------
2024-04-03 19:46:41,190 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings(
      'glove'
      (embedding): Embedding(400001, 100)
    )
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)
  (rnn): LSTM(4196, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=7, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2024-04-03 19:46:41,190 ----------------------------------------------------------------------------------------------------
2024-04-03 19:46:41,191 Corpus: 34308 train + 4384 dev + 4384 test sentences
2024-04-03 19:46:41,191 ----------------------------------------------------------------------------------------------------
2024-04-03 19:46:41,192 Train:  34308 sentences
2024-04-03 19:46:41,193         (train_with_dev=False, train_with_test=False)
2024-04-03 19:46:41,193 ----------------------------------------------------------------------------------------------------
2024-04-03 19:46:41,194 Training Params:
2024-04-03 19:46:41,194  - learning_rate: "0.05" 
2024-04-03 19:46:41,195  - mini_batch_size: "32"
2024-04-03 19:46:41,196  - max_epochs: "20"
2024-04-03 19:46:41,196  - shuffle: "True"
2024-04-03 19:46:41,196 ----------------------------------------------------------------------------------------------------
2024-04-03 19:46:41,197 Plugins:
2024-04-03 19:46:41,197  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2024-04-03 19:46:41,198 ----------------------------------------------------------------------------------------------------
2024-04-03 19:46:41,198 Final evaluation on model from best epoch (best-model.pt)
2024-04-03 19:46:41,198  - metric: "('micro avg', 'f1-score')"
2024-04-03 19:46:41,199 ----------------------------------------------------------------------------------------------------
2024-04-03 19:46:41,199 Computation:
2024-04-03 19:46:41,199  - compute on device: cpu
2024-04-03 19:46:41,200  - embedding storage: cpu
2024-04-03 19:46:41,200 ----------------------------------------------------------------------------------------------------
2024-04-03 19:46:41,200 Model training base path: "test-flair"
2024-04-03 19:46:41,201 ----------------------------------------------------------------------------------------------------
2024-04-03 19:46:41,201 ----------------------------------------------------------------------------------------------------
2024-04-03 19:55:16,366 epoch 1 - iter 107/1073 - loss 0.46667242 - time (sec): 515.16 - samples/sec: 110.85 - lr: 0.050000 - momentum: 0.000000
