{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq: incorrect sentences -> correct sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def apply_corrections(sentence, corrections):\n",
    "    \"\"\"\n",
    "    Apply corrections to the original sentence.\n",
    "    Corrections are provided as a list of edit operations.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for correction in corrections[::-1]:\n",
    "        parts = correction.split(\"|||\")\n",
    "        span = parts[0].split()[1:]  # The span to be corrected\n",
    "        start, end = int(span[0]), int(span[1])\n",
    "        correction_text = parts[2]  # The text to replace the span\n",
    "        \n",
    "        if start == -1 and end == -1:  # Special case for sentence-level corrections\n",
    "            continue\n",
    "        elif correction_text == \"-NONE-\":  # Deletion\n",
    "            words[start:end] = []\n",
    "        else:  # Replacement or insertion\n",
    "            words[start:end] = correction_text.split()\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "def extract_correct_sentences_from_m2(filepath, output_file):\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    sentence_pairs = []\n",
    "    current_sentence = \"\"\n",
    "    corrections = []\n",
    "    for line in lines:\n",
    "        if line.startswith('S '):  # Start of a new sentence\n",
    "            if current_sentence:  # Process the previous sentence\n",
    "                corrected_sentence = apply_corrections(current_sentence, corrections)\n",
    "                # sentence_pairs.append({\"src\": current_sentence, \"tgt\": corrected_sentence})\n",
    "                sentence_pairs.append({\"translation\": {\"src\": current_sentence, \"tgt\": corrected_sentence}})\n",
    "            current_sentence = line[2:].strip()  # Update current sentence\n",
    "            corrections = []  # Reset corrections\n",
    "        elif line.startswith('A '):  # Correction for the current sentence\n",
    "            corrections.append(line.strip())\n",
    "\n",
    "    # Don't forget to process the last sentence\n",
    "    if current_sentence:\n",
    "        corrected_sentence = apply_corrections(current_sentence, corrections)\n",
    "        # sentence_pairs.append({\"src\": current_sentence, \"tgt\": corrected_sentence})\n",
    "        sentence_pairs.append({\"translation\": {\"src\": current_sentence, \"tgt\": corrected_sentence}})\n",
    "\n",
    "    # Save the sentence pairs to a JSON file\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        # json.dump(sentence_pairs, out_file, indent=2)\n",
    "        for d in sentence_pairs:\n",
    "            out_file.write(json.dumps(d, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected sentences have been saved to ./wi+locness/seq2seq_t5/N.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/seq2seq_t5/ABCN.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/seq2seq_t5/A.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/seq2seq_t5/B.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/seq2seq_t5/C.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/seq2seq_t5/B.train.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/seq2seq_t5/C.train.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/seq2seq_t5/ABC.train.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/seq2seq_t5/A.train.gold.bea19.json\n"
     ]
    }
   ],
   "source": [
    "# m2_filepath = './wi+locness/m2/A.dev.gold.bea19.m2'  # Adjust to your M2 file's location\n",
    "# output_filepath = './wi+locness/seq2seq/A.dev.gold.bea19.json'  # Output file for corrected sentences\n",
    "# extract_correct_sentences_from_m2(m2_filepath, output_filepath)\n",
    "# print(f\"Corrected sentences have been saved to {output_filepath}\")\n",
    "\n",
    "import os\n",
    "# walk through the directory\n",
    "path = './wi+locness/m2/'\n",
    "for foldername, subfolders, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.m2'):\n",
    "            m2_filepath = os.path.join(foldername, filename)\n",
    "            output_filepath = os.path.join(foldername.replace('m2','seq2seq_t5'), filename.replace('.m2', '.json'))\n",
    "            extract_correct_sentences_from_m2(m2_filepath, output_filepath)\n",
    "            print(f\"Corrected sentences have been saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq: template -> correct sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def apply_templates(sentence, corrections):\n",
    "    \"\"\"\n",
    "    Apply corrections to the original sentence.\n",
    "    Corrections are provided as a list of edit operations.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Apply corrections to the original sentence.\n",
    "    Corrections are provided as a list of edit operations.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    template = []\n",
    "    tmp = []\n",
    "    for i, correction in enumerate(corrections):\n",
    "        parts = correction.split(\"|||\")\n",
    "        span = parts[0].split()[1:]  # The span to be corrected\n",
    "        start, end = int(span[0]), int(span[1])\n",
    "        correction_text = parts[2]  # The text to replace the span\n",
    "        \n",
    "        if start == -1 and end == -1:  # Special case for sentence-level corrections\n",
    "            continue\n",
    "        else:  \n",
    "            if start==end:\n",
    "                end += 1\n",
    "            template.append(f'<extra_id_{i}>')\n",
    "            template.extend(words[start:end])\n",
    "            tmp.append((start,end))\n",
    "\n",
    "    for i, t in enumerate(tmp[::-1]):\n",
    "        start, end = t\n",
    "        words[start:end] = f'<extra_id_{len(tmp)-1-i}>'.split()\n",
    "           \n",
    "            \n",
    "    output = f\"{' '.join(template)} </s> {' '.join(words)}\"\n",
    "    # print(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def apply_corrections(sentence, corrections):\n",
    "    \"\"\"\n",
    "    Apply corrections to the original sentence.\n",
    "    Corrections are provided as a list of edit operations.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for correction in corrections[::-1]:\n",
    "        parts = correction.split(\"|||\")\n",
    "        span = parts[0].split()[1:]  # The span to be corrected\n",
    "        start, end = int(span[0]), int(span[1])\n",
    "        correction_text = parts[2]  # The text to replace the span\n",
    "        \n",
    "        if start == -1 and end == -1:  # Special case for sentence-level corrections\n",
    "            continue\n",
    "        elif correction_text == \"-NONE-\":  # Deletion\n",
    "            words[start:end] = []\n",
    "        else:  # Replacement or insertion\n",
    "            words[start:end] = correction_text.split()\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "def extract_correct_sentences_from_m2(filepath, output_file):\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    sentence_pairs = []\n",
    "    current_sentence = \"\"\n",
    "    corrections = []\n",
    "    for line in lines:\n",
    "        if line.startswith('S '):  # Start of a new sentence\n",
    "            if current_sentence:  # Process the previous sentence\n",
    "                corrected_sentence = apply_corrections(current_sentence, corrections)\n",
    "                template_sentence = apply_templates(current_sentence, corrections)\n",
    "                # sentence_pairs.append({\"src\": template_sentence, \"tgt\": corrected_sentence})\n",
    "                sentence_pairs.append({\"translation\": {\"src\": template_sentence, \"tgt\": corrected_sentence}})\n",
    "            current_sentence = line[2:].strip()  # Update current sentence\n",
    "            corrections = []  # Reset corrections\n",
    "        elif line.startswith('A '):  # Correction for the current sentence\n",
    "            corrections.append(line.strip())\n",
    "\n",
    "    # Don't forget to process the last sentence\n",
    "    if current_sentence:\n",
    "        corrected_sentence = apply_corrections(current_sentence, corrections)\n",
    "        template_sentence = apply_templates(current_sentence, corrections)\n",
    "        # sentence_pairs.append({\"src\": template_sentence, \"tgt\": corrected_sentence})\n",
    "        sentence_pairs.append({\"translation\": {\"src\": template_sentence, \"tgt\": corrected_sentence}})\n",
    "\n",
    "    # Save the sentence pairs to a JSON file\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        # json.dump(sentence_pairs, out_file, indent=2)\n",
    "        for d in sentence_pairs:\n",
    "            out_file.write(json.dumps(d, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected sentences have been saved to ./wi+locness/template2seq_2class_t5/N.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_2class_t5/ABCN.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_2class_t5/A.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_2class_t5/B.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_2class_t5/C.dev.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_2class_t5/B.train.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_2class_t5/C.train.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_2class_t5/ABC.train.gold.bea19.json\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_2class_t5/A.train.gold.bea19.json\n"
     ]
    }
   ],
   "source": [
    "# m2_filepath = './wi+locness/m2/A.dev.gold.bea19.m2'  # Adjust to your M2 file's location\n",
    "# output_filepath = './wi+locness/template2seq/A.dev.gold.bea19.json'  # Output file for corrected sentences\n",
    "# extract_correct_sentences_from_m2(m2_filepath, output_filepath)\n",
    "# print(f\"Corrected sentences have been saved to {output_filepath}\")\n",
    "\n",
    "import os\n",
    "# walk through the directory\n",
    "path = './wi+locness/m2/'\n",
    "for foldername, subfolders, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.m2'):\n",
    "            m2_filepath = os.path.join(foldername, filename)\n",
    "            output_filepath = os.path.join(foldername.replace('m2','template2seq_2class_t5'), filename.replace('.m2', '.json'))\n",
    "            extract_correct_sentences_from_m2(m2_filepath, output_filepath)\n",
    "            print(f\"Corrected sentences have been saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def apply_templates(sentence, corrections):\n",
    "    \"\"\"\n",
    "    Apply corrections to the original sentence.\n",
    "    Corrections are provided as a list of edit operations.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Apply corrections to the original sentence.\n",
    "    Corrections are provided as a list of edit operations.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    template = []\n",
    "    tmp = []\n",
    "    for i, correction in enumerate(corrections):\n",
    "        parts = correction.split(\"|||\")\n",
    "        span = parts[0].split()[1:]  # The span to be corrected\n",
    "        start, end = int(span[0]), int(span[1])\n",
    "        correction_type = parts[1]  # The type of correction\n",
    "        correction_type = correction_type.split(':')[0]\n",
    "        correction_text = parts[2]  # The text to replace the span\n",
    "        \n",
    "        if start == -1 and end == -1:  # Special case for sentence-level corrections\n",
    "            continue\n",
    "        else:  \n",
    "            if start==end:\n",
    "                end += 1\n",
    "            template.append(f'<extra_id_{i}> {correction_type}:')\n",
    "            template.extend(words[start:end])\n",
    "            tmp.append((start,end))\n",
    "\n",
    "    for i, t in enumerate(tmp[::-1]):\n",
    "        start, end = t\n",
    "        words[start:end] = f'<extra_id_{len(tmp)-1-i}>'.split()\n",
    "           \n",
    "            \n",
    "    output = f\"{' '.join(template)} </s> {' '.join(words)}\"\n",
    "    # print(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def apply_corrections(sentence, corrections):\n",
    "    \"\"\"\n",
    "    Apply corrections to the original sentence.\n",
    "    Corrections are provided as a list of edit operations.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for correction in corrections[::-1]:\n",
    "        parts = correction.split(\"|||\")\n",
    "        span = parts[0].split()[1:]  # The span to be corrected\n",
    "        start, end = int(span[0]), int(span[1])\n",
    "        correction_text = parts[2]  # The text to replace the span\n",
    "        \n",
    "        if start == -1 and end == -1:  # Special case for sentence-level corrections\n",
    "            continue\n",
    "        elif correction_text == \"-NONE-\":  # Deletion\n",
    "            words[start:end] = []\n",
    "        else:  # Replacement or insertion\n",
    "            words[start:end] = correction_text.split()\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "def extract_correct_sentences_from_m2(filepath, output_file):\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    sentence_pairs = []\n",
    "    current_sentence = \"\"\n",
    "    length = 0\n",
    "    corrections = []\n",
    "    for line in lines:\n",
    "        if line.startswith('S '):  # Start of a new sentence\n",
    "            if current_sentence:  # Process the previous sentence\n",
    "                corrected_sentence = apply_corrections(current_sentence, corrections)\n",
    "                template_sentence = apply_templates(current_sentence, corrections)\n",
    "                # sentence_pairs.append({\"src\": template_sentence, \"tgt\": corrected_sentence})\n",
    "                sentence_pairs.append({\"translation\": {\"src\": template_sentence, \"tgt\": corrected_sentence}})\n",
    "            current_sentence = line[2:].strip()  # Update current sentence\n",
    "            length = max(length, len(current_sentence))\n",
    "            corrections = []  # Reset corrections\n",
    "        elif line.startswith('A '):  # Correction for the current sentence\n",
    "            corrections.append(line.strip())\n",
    "\n",
    "    # Don't forget to process the last sentence\n",
    "    if current_sentence:\n",
    "        corrected_sentence = apply_corrections(current_sentence, corrections)\n",
    "        template_sentence = apply_templates(current_sentence, corrections)\n",
    "        # sentence_pairs.append({\"src\": template_sentence, \"tgt\": corrected_sentence})\n",
    "        sentence_pairs.append({\"translation\": {\"src\": template_sentence, \"tgt\": corrected_sentence}})\n",
    "    # print(length)\n",
    "        \n",
    "    # Save the sentence pairs to a JSON file\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        # json.dump(sentence_pairs, out_file, indent=2)\n",
    "        for d in sentence_pairs:\n",
    "            out_file.write(json.dumps(d, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_4class_t5/N.dev.gold.bea19.json\n",
      "790\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_4class_t5/ABCN.dev.gold.bea19.json\n",
      "754\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_4class_t5/A.dev.gold.bea19.json\n",
      "647\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_4class_t5/B.dev.gold.bea19.json\n",
      "790\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_4class_t5/C.dev.gold.bea19.json\n",
      "751\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_4class_t5/B.train.gold.bea19.json\n",
      "652\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_4class_t5/C.train.gold.bea19.json\n",
      "1094\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_4class_t5/ABC.train.gold.bea19.json\n",
      "1094\n",
      "Corrected sentences have been saved to ./wi+locness/template2seq_4class_t5/A.train.gold.bea19.json\n"
     ]
    }
   ],
   "source": [
    "# m2_filepath = './wi+locness/m2/A.dev.gold.bea19.m2'  # Adjust to your M2 file's location\n",
    "# output_filepath = './wi+locness/template2seq/A.dev.gold.bea19.json'  # Output file for corrected sentences\n",
    "# extract_correct_sentences_from_m2(m2_filepath, output_filepath)\n",
    "# print(f\"Corrected sentences have been saved to {output_filepath}\")\n",
    "\n",
    "import os\n",
    "# walk through the directory\n",
    "path = './wi+locness/m2/'\n",
    "for foldername, subfolders, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.m2'):\n",
    "            m2_filepath = os.path.join(foldername, filename)\n",
    "            output_filepath = os.path.join(foldername.replace('m2','template2seq_4class_t5'), filename.replace('.m2', '.json'))\n",
    "            extract_correct_sentences_from_m2(m2_filepath, output_filepath)\n",
    "            print(f\"Corrected sentences have been saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
